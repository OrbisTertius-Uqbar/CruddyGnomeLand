\documentclass[12pt]{report}
\author{Cameron Rudd \& Avinoam Henig}
\usepackage{fullpage, setspace, cite}
\usepackage{pgfplotstable}
\usepackage{pgfplots}
\doublespacing
\title{Learning and Evolution in Simulated World}

\pgfplotsset{compat=1.8}
\begin{document}



\maketitle
\abstract 
Inspired by the work of Ackley and Littman \cite{erl}, a two dimensional tile world inhabited by self directed creatures was implemented in python and run for variable lengths of time. The behavior of the creatures was then analyzed in order to determine whether learned strategies and evolution caused older, larger populations, and speciation. Various control factors were altered between runs in an attempt to encourage certain behaviors. Each creature contained two neural networks, an evaluation network and an action network. The action network determined which of several actions the creature would make on a given turn. This network was improved each time step using complementary reinforced back propagation. This was implemented using the evaluation network to determine whether the creature had made a good or bad decision. The creatures had the ability to self reproduce, reproduce with others, and die. 

\section*{Background}

\paragraph{} In their 1991 paper \textit{Interactions Between Learning and Evolution} \cite{erl} Ackley and Littman investigated whether a learning algorithm could learn ``given only natural selection as feedback." In the paper, Ackley and Littman outline evolutionary reinforcement learning ($ERL$), which combines traditional neural network learning with genetic evolutionary approaches. The approach uses an artificial ecosystem in which $ERL$ driven agents interact with the ecosystem, trying to prolong their ``life" and ``survive".
\paragraph{} In Ackley and Littman's world, called $AL$, uses a $100\times 100$ two dimensional array of cells inhabited by various landscape features, adaptive lifeforms and non adaptive lifeforms. Different objects in the world have varying affects on the $ERL$ agents. For example, a carnivore eat action hurts an $ERL$ agent while an $ERL$ agent eat action on a plant increases the health of the agent. 
\paragraph{} The $ERL$ agents are able to ``see" the closest world object that is within four cells of itself. The agents die when their health and energy is low and reproduce when it is high. Reproduction can be done solely by an agent, whereby it mutates its own genome, or with another nearby agent, whereby the two agent's genomes are crossed over and then mutated.
\paragraph{} Ackley and Littman's work was primarily a proof of concept; they demonstrated that learning can occur when given only life and death as feedback. Furthermore, Ackley and Littman found that in their world, the $ERL$ approach was found to more successful then either a pure evolutionary approach or a pure learning approach. Interestingly, however, Ackley and Littman  found that when compared to agents that behaved randomly, the $ERL$ agents only outperformed the random agents after 500,000 time steps. 

\section*{Methods}
\paragraph{} We implemented our artificial ecosystem, called Cruddy Gnome Land, in python. The world, which wrapped around, contained 3,375 tiles arranged in a $75 \times 45$ two-dimensional array. Each tile could contain a tree, a plant, or a creature. A creature moving into a tile containing a plant resulted in the creature eating the plant and the creature's health increasing. Attempting to move into a cell containing a tree negatively impacted the creature's health. Were a creature to attempt to move into a tile occupied by another creature, a probabilistic ``success" test was called. This test used the health values of the two creatures to determine whether the move into the occupied tile was successful. When the move resulted in success, the moving creature consumed the other creature and gained health. When the move failed, the moving creature's health decreased.
\paragraph{}  Creatures were equipped with evaluation and action neural networks. The action network took sensory input and output an action. The evaluation network sensory input from after the creature's action. It then output a scalar corresponding to whether the action improved or worsened the state of the creature. Both networks took thirteen inputs, twelve corresponding to sensory input (sight); the thirteenth corresponded to the health of the creature. The networks were built using the CONX module. The action network was trained over the life of the creature using the complimentary back propagation algorithm described by Ackley and Littman in \cite{erl}. 
\paragraph{} Each creature in the world possessed a genome containing the weights for its action and evaluation networks. Initially, like Ackley and Littman, the genome was fixed for the life of the creature. However later on we departed from Ackley and Littman and tested an epigenetic feature that allowed creatures to pass on the learned weights of their action network. This was done by updating the genome of the creature at reproduction.  
\paragraph{} A graphical representation of the world was built using Pyglet and a database containing all creature data was built using MondoDB. Additionally, a command line interface was developed using python's multiprocessing module.
\section*{Results}
The following data is from a 68551 time step of Cruddy Gnome Land. The first plot shows both the average age (in green) of every creature in the world and the age of the oldest creature in the world (in red). The second plot shows the size of the current population (in blue).

\begin{center}
\pgfplotstableread{rundata.dat}{\data}
\begin{tikzpicture}[scale=1]
\begin{axis}[width = 1\textwidth, height = 0.5 \textwidth, title=Cruddy Gnome Land Run 1,
	xlabel=Time Step, ymin = 0, xmin = 0, xmax = 70000, ylabel =  Age (in time steps alive)]
\addplot [green, very thick] table [x={time}, y={avgAge}] {\data};
\addplot [red,very thick] table [x={time}, y={maxAge}] {\data};
\end{axis}
\end{tikzpicture}
\end{center}

\begin{center}
\pgfplotstableread{rundata.dat}{\data}
\begin{tikzpicture}[scale=1]
\begin{axis}[width = 1\textwidth, height = 0.5 \textwidth, title=Cruddy Gnome Land Run 1,
	xlabel=Time Step, ymin = 0, xmin = 0, xmax = 70000, ylabel =  Population Size]
\addplot [blue, very thick] table [x={time}, y={alive}] {\data};
\end{axis}
\end{tikzpicture}
\end{center}

\section*{Conclusions}
\paragraph{} 
We spent a good deal of time developing highly flexible code. As a result, the code developed during this project is very general and with little modification could be packaged as a general purpose microworld  library. This library could be used to create a variety biologically inspired environments that evolve in realtime. Because of the databasing features, data from these worlds could be easily accessed and immediately analyzed.  
\paragraph{} Having primarily focused on developing our code instead of experimenting with the world, there are a variety of ideas we are interested in exploring that we did not have the opportunity to test. We are especially interested in speciation and several ideas for how to encourage the emergence of speciation. One idea involves tagging each creature in the initial population with a random boolean value. The boolean value would be carried in the genome and creatures would only be able to reproduce with other creatures that have the same tag. This would lead to two divergent populations that would coevolve. Another idea was to create physical specifications for each creature and encoding the specifications in the genome. This would be down by allocating ``points" to different attributes. As each creature has a finite number of points, different creatures could emphasize different attributes. For example one creature may have very high health but not be able to move quickly or see very far. Another creature, though, could be fast and see far, but have lower maximum health. These two example creatures could develop a predator prey dynamic.
\end{document}